{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj-Yathinatti/KDDM2/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-QarW6R75o3"
      },
      "source": [
        "# Advanced Information Retrieval - Homework 2\n",
        "\n",
        "### Organizational infos:\n",
        "* 30 Points in total \n",
        "* Submission: ipynb file + pdf(File -> Print preview -> Print Website)\n",
        "* Deadline: 20.12.2022 11:00\n",
        "* Please use the Teach Center to ask questions\n",
        "\n",
        "\n",
        "\n",
        "### General information:\n",
        "Within this homework you are asked to solve practical tasks and answer theoretical questions inbetween. \n",
        "* replace **[your answer]** with your actual answer\n",
        "* usage of specific documents/words are index based -> e.g. document 3 = docs[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QRBRe55k75o-"
      },
      "outputs": [],
      "source": [
        "#imports \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import ast\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "# #add further needed libraries below\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# import gensim\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I_R9rM675o_"
      },
      "source": [
        "## Word2Vec\n",
        "In this task you are asked to get familiar with word2vec by performing some simple similarity tasks. The training is already done and we provide you the trained word vectors. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJDsVrXG75o_"
      },
      "source": [
        "**1: What assumption (hypothesis) is Word2Vec based on? [1 Point]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg9TL3gH75pA"
      },
      "source": [
        "It is bases on maximize log probability of any context word given the current center word optmised via gradient descent. Bascially predict the words that will be in each word's m-sized window.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEvQxfg475pA"
      },
      "source": [
        "**2: Loading word2Vec model. [2 Points]**\n",
        "Here you can load your word2vec model, by using the pretrained word vectors **corpus_homework2.txt**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7_Hc0kF_w2w",
        "outputId": "b1a458b4-05ec-4dff-e4e8-aede97b76cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST0tylqa75pA"
      },
      "outputs": [],
      "source": [
        "#load/setup your word2vec \n",
        "#your code\n",
        "path = \"/content/drive/MyDrive/Course/AIR/data/w2vdata/\"\n",
        "corups = pd.read_table(path + \"corpus_homework2.txt\", sep=\" \", skiprows=1, header=None)\n",
        "corups.to_csv(path+\"corups.csv\", index = False)\n",
        "\n",
        "wrd = corups[corups[0] == 'word'].values.tolist()[0][1:]\n",
        "\n",
        "# #loading data on which the model is trained on\n",
        "df = pd.read_csv(path + 'homework2_dataset.csv')\n",
        "df['txt'] = df['txt'].apply(ast.literal_eval)\n",
        "docs = [col for col in df['txt']]\n",
        "\n",
        "\n",
        "# for my understanding docs contains nested list [[\"a\", \"b\"], [\"c\", \"d\"], [\"e\", \"d\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imHnLmQs75pB"
      },
      "outputs": [],
      "source": [
        "#init mtnr\n",
        "mtnr = 12116972\n",
        "np.random.seed(mtnr)\n",
        "random.seed(mtnr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhXU32JQ75pB"
      },
      "source": [
        "**3: Calculate the similarity of word $0$ and $1$ within the given document. [1 Point]**\n",
        "Use the word2vec model to compute the similarity between both words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUQnU7jm75pC",
        "outputId": "c2634824-d5d9-47c4-df93-9421008acbb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using [0.007560634, -0.1130593, 0.008335069, -0.060224008, 0.044669297, -0.13517031, 0.0845674, 0.19454256, -0.19937466, -0.14181122, 0.00859519, 0.009864115, 0.062566064, 0.14316303, -0.111513525, 0.23961712, 0.062656485, 0.03506885, -0.09704982, 0.13627177, 0.011750163, 0.14152952, 0.019195724, 0.03282841, 0.19611993] \n",
            " distbelief [0.0025297788, 0.021401228, -0.009025138, -0.01708257, -0.00471992, -0.05038602, 0.03866974, -0.017468428, -0.0001411171, -0.016988963, -0.01580178, 0.018699754, -0.0011827943, 0.018799758, 0.027891602, 0.047245823, -0.0047104526, -0.0060859877, -0.004245897, 0.0069290455, 0.018351745, 0.025361251, -0.018239314, 0.04701092, 0.030340843]\n",
            "\n",
            "similarity:  0.4434885029928691\n"
          ]
        }
      ],
      "source": [
        "#similarity\n",
        "doc_sim = docs[np.random.randint(len(docs))]\n",
        "word0 = doc_sim[0]\n",
        "word1 = doc_sim[1]\n",
        "# your code\n",
        "\n",
        "wrd0 = corups[corups[0] == word0].values.tolist()[0][1:]\n",
        "wrd1 = corups[corups[0] == word1].values.tolist()[0][1:]\n",
        "print(word0, wrd0, \"\\n\", word1, wrd1)\n",
        "\n",
        "similarity = np.dot(wrd0, wrd1)/(np.linalg.norm(wrd0)* np.linalg.norm(wrd1))\n",
        "print(\"\\nsimilarity: \",similarity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCN_QUiG75pC"
      },
      "source": [
        "**4: Retrieve the 5 most similar words of the complete corpus to the random selected word. [1 Point]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MANdbTkw75pD",
        "outputId": "be19acc3-ab9c-4869-f509-734a262f8d0b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'end'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Random word:\n",
        "random.seed(mtnr)\n",
        "random_word = random.choice(docs[np.random.randint(len(docs))])\n",
        "random_word\n",
        "\n",
        "# top = most_similar(positive=random_word, topn=5)\n",
        "# sim_words = corups.similar_by_word(word = random_word, topn=5)\n",
        "# print(sim_words)\n",
        "# Word2Vec.most_similar(random_word, topn=5)\n",
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuuLHnaF75pD"
      },
      "source": [
        "**5: Calculate the similarity between documents $0$ and $1$. [1 Point]**\n",
        "Convert your documents into vector representations using the corpus file and then compute their (cosine) similarity using average pooling (average of all words in a document)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE2gAbIk21QV",
        "outputId": "bf6f5f14-3ad0-4025-ad8f-3ff75bb064bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['quality', 'representations', 'measured', 'word', 'similarity', 'task', 'results', 'compared', 'previously', 'best', 'performing', 'techniques', 'based', 'different', 'types', 'neural', 'networks']\n"
          ]
        }
      ],
      "source": [
        "for d in docs:\n",
        "  print(d)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "VQ_fUGtK75pD",
        "outputId": "b295d4c3-b597-4a7b-bd26-79a986e64068"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d87b0e17660b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdoc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc0:  \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\ndoc1:  \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "doc2vec = None # TODO\n",
        "\n",
        "doc0 = doc2vec[0]\n",
        "doc1 = doc2vec[1]\n",
        "print(\"doc0:  \", doc0, \"\\ndoc1:  \",doc1)\n",
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee1qIzBD75pD"
      },
      "source": [
        "**6: Find the 3 most similar documents based on the given query. [1 Point]**\n",
        "The given query necessarily has to be preprocessed to be used in your word2vec model. Again, use average pooling to represent the documents and query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ0gaVe675pE"
      },
      "outputs": [],
      "source": [
        "query = \"information retrieval\" \n",
        "\n",
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0vbJJ0r75pE"
      },
      "source": [
        "**7: Given a query and 3 documents. Find the best one word expansion query for these docs. [1 Point]**\n",
        "* e.g. query  = \"word1\" should become: query=\"word1 word2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9gb70fG75pE"
      },
      "outputs": [],
      "source": [
        "random.seed(mtnr)\n",
        "docs = random.choices(docs, k=3)\n",
        "query = random.choice(random.choice(docs))\n",
        "\n",
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_3Yfp0775pE"
      },
      "source": [
        "## SkipGram - Own training and test loop implementation\n",
        "\n",
        "In this section you are asked to implement your own training and test loop for the SkipGram model. The model is already provided and is based on the torch nn module. Furthermore, you also need to bring the given dataset into a usable format.\n",
        "\n",
        "\n",
        "\n",
        "For this task we would recommend the following resources:\n",
        "* [Pytorch Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) \n",
        "* [DataLoader](https://pytorch.org/docs/stable/data.html)\n",
        "* [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "* [Optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
        "\n",
        "Note: small mistakes won't weight heavy. The goal is to understand the logic behind training and testing loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebZbenme75pF"
      },
      "source": [
        "**8: Prepare the given dataset and split it into a training and test set (80/20). [3 Points]**\n",
        "\n",
        "1. Create a lookup dictionary where each unique word(token) gets assigned with an integer index\n",
        "2. Replace your words(tokens) in the documents with the values from the lookup dict. \n",
        "        2.1. for e.g. [word0, word1, word2] would look like [110,25,360]\n",
        "3. Split your documents into training/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiIPp-Zy75pF",
        "outputId": "d8fa7d82-6c7b-4fa1-8e33-be0cbe72a1dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['quality',\n",
              "  'representations',\n",
              "  'measured',\n",
              "  'word',\n",
              "  'similarity',\n",
              "  'task',\n",
              "  'results',\n",
              "  'compared',\n",
              "  'previously',\n",
              "  'best',\n",
              "  'performing',\n",
              "  'techniques',\n",
              "  'based',\n",
              "  'different',\n",
              "  'types',\n",
              "  'neural',\n",
              "  'networks'],\n",
              " ['observe',\n",
              "  'large',\n",
              "  'improvements',\n",
              "  'accuracy',\n",
              "  'much',\n",
              "  'lower',\n",
              "  'computational',\n",
              "  'cost',\n",
              "  'ie',\n",
              "  'takes',\n",
              "  'less',\n",
              "  'day',\n",
              "  'learn',\n",
              "  'high',\n",
              "  'quality',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  '16',\n",
              "  'billion',\n",
              "  'words',\n",
              "  'data',\n",
              "  'set'],\n",
              " ['furthermore',\n",
              "  'show',\n",
              "  'vectors',\n",
              "  'provide',\n",
              "  'stateoftheart',\n",
              "  'performance',\n",
              "  'test',\n",
              "  'set',\n",
              "  'measuring',\n",
              "  'syntactic',\n",
              "  'semantic',\n",
              "  'word',\n",
              "  'similarities'],\n",
              " ['introduction',\n",
              "  'many',\n",
              "  'current',\n",
              "  'nlp',\n",
              "  'systems',\n",
              "  'techniques',\n",
              "  'treat',\n",
              "  'words',\n",
              "  'atomic',\n",
              "  'units',\n",
              "  'notion',\n",
              "  'similarity',\n",
              "  'words',\n",
              "  'represented',\n",
              "  'indices',\n",
              "  'vocabulary'],\n",
              " ['choice',\n",
              "  'several',\n",
              "  'good',\n",
              "  'reasons',\n",
              "  'simplicity',\n",
              "  'robustness',\n",
              "  'observation',\n",
              "  'simple',\n",
              "  'models',\n",
              "  'trained',\n",
              "  'huge',\n",
              "  'amounts',\n",
              "  'data',\n",
              "  'outperform',\n",
              "  'complex',\n",
              "  'systems',\n",
              "  'trained',\n",
              "  'less',\n",
              "  'data'],\n",
              " ['however', 'simple', 'techniques', 'limits', 'many', 'tasks'],\n",
              " ['example',\n",
              "  'amount',\n",
              "  'relevant',\n",
              "  'indomain',\n",
              "  'data',\n",
              "  'automatic',\n",
              "  'speech',\n",
              "  'recognition',\n",
              "  'limited',\n",
              "  'performance',\n",
              "  'usually',\n",
              "  'dominated',\n",
              "  'size',\n",
              "  'high',\n",
              "  'quality',\n",
              "  'transcribed',\n",
              "  'speech',\n",
              "  'data',\n",
              "  'often',\n",
              "  'millions',\n",
              "  'words'],\n",
              " ['machine',\n",
              "  'translation',\n",
              "  'existing',\n",
              "  'corpora',\n",
              "  'many',\n",
              "  'languages',\n",
              "  'contain',\n",
              "  'billions',\n",
              "  'words',\n",
              "  'less'],\n",
              " ['thus',\n",
              "  'situations',\n",
              "  'simple',\n",
              "  'scaling',\n",
              "  'basic',\n",
              "  'techniques',\n",
              "  'result',\n",
              "  'signiﬁcant',\n",
              "  'progress',\n",
              "  'focus',\n",
              "  'advanced',\n",
              "  'techniques'],\n",
              " ['progress',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'techniques',\n",
              "  'recent',\n",
              "  'years',\n",
              "  'become',\n",
              "  'possible',\n",
              "  'train',\n",
              "  'complex',\n",
              "  'models',\n",
              "  'much',\n",
              "  'larger',\n",
              "  'data',\n",
              "  'set',\n",
              "  'typically',\n",
              "  'outperform',\n",
              "  'simple',\n",
              "  'models'],\n",
              " ['main',\n",
              "  'goal',\n",
              "  'paper',\n",
              "  'introduce',\n",
              "  'techniques',\n",
              "  'used',\n",
              "  'learning',\n",
              "  'highquality',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'huge',\n",
              "  'data',\n",
              "  'sets',\n",
              "  'billions',\n",
              "  'words',\n",
              "  'millions',\n",
              "  'words',\n",
              "  'vocabulary'],\n",
              " ['somewhat',\n",
              "  'surprisingly',\n",
              "  'found',\n",
              "  'similarity',\n",
              "  'word',\n",
              "  'representations',\n",
              "  'goes',\n",
              "  'beyond',\n",
              "  'simple',\n",
              "  'syntactic',\n",
              "  'regularities'],\n",
              " ['paper',\n",
              "  'try',\n",
              "  'maximize',\n",
              "  'accuracy',\n",
              "  'vector',\n",
              "  'operations',\n",
              "  'developing',\n",
              "  'new',\n",
              "  'model',\n",
              "  'architectures',\n",
              "  'preserve',\n",
              "  'linear',\n",
              "  'regularities',\n",
              "  'among',\n",
              "  'words'],\n",
              " ['design',\n",
              "  'new',\n",
              "  'comprehensive',\n",
              "  'test',\n",
              "  'set',\n",
              "  'measuring',\n",
              "  'syntactic',\n",
              "  'semantic',\n",
              "  'regularities',\n",
              "  'show',\n",
              "  'many',\n",
              "  'regularities',\n",
              "  'learned',\n",
              "  'high',\n",
              "  'accuracy'],\n",
              " ['moreover',\n",
              "  'discuss',\n",
              "  'training',\n",
              "  'time',\n",
              "  'accuracy',\n",
              "  'depends',\n",
              "  'dimensionality',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'amount',\n",
              "  'training',\n",
              "  'data'],\n",
              " ['thus',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'learned',\n",
              "  'even',\n",
              "  'without',\n",
              "  'constructing',\n",
              "  'full',\n",
              "  'nnlm'],\n",
              " ['work',\n",
              "  'directly',\n",
              "  'extend',\n",
              "  'architecture',\n",
              "  'focus',\n",
              "  'ﬁrst',\n",
              "  'step',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'learned',\n",
              "  'using',\n",
              "  'simple',\n",
              "  'model'],\n",
              " ['model',\n",
              "  'architectures',\n",
              "  'many',\n",
              "  'different',\n",
              "  'types',\n",
              "  'models',\n",
              "  'proposed',\n",
              "  'estimating',\n",
              "  'continuous',\n",
              "  'representations',\n",
              "  'words',\n",
              "  'including',\n",
              "  'wellknown',\n",
              "  'latent',\n",
              "  'semantic',\n",
              "  'analysis',\n",
              "  'lsa',\n",
              "  'latent',\n",
              "  'dirichlet',\n",
              "  'allocation',\n",
              "  'lda'],\n",
              " ['next',\n",
              "  'try',\n",
              "  'maximize',\n",
              "  'accuracy',\n",
              "  'minimizing',\n",
              "  'computational',\n",
              "  'complexity'],\n",
              " ['input',\n",
              "  'layer',\n",
              "  'previous',\n",
              "  'words',\n",
              "  'encoded',\n",
              "  'using',\n",
              "  '1of',\n",
              "  'vcoding',\n",
              "  'size',\n",
              "  'vocabulary'],\n",
              " ['input',\n",
              "  'layer',\n",
              "  'projected',\n",
              "  'projection',\n",
              "  'layer',\n",
              "  'pthat',\n",
              "  'dimensionality',\n",
              "  'n\\x02d',\n",
              "  'using',\n",
              "  'shared',\n",
              "  'projection',\n",
              "  'matrix'],\n",
              " ['inputs',\n",
              "  'active',\n",
              "  'given',\n",
              "  'time',\n",
              "  'composition',\n",
              "  'projection',\n",
              "  'layer',\n",
              "  'relatively',\n",
              "  'cheap',\n",
              "  'operation'],\n",
              " ['nnlm',\n",
              "  'architecture',\n",
              "  'becomes',\n",
              "  'complex',\n",
              "  'computation',\n",
              "  'projection',\n",
              "  'hidden',\n",
              "  'layer',\n",
              "  'values',\n",
              "  'projection',\n",
              "  'layer',\n",
              "  'dense'],\n",
              " ['binary',\n",
              "  'tree',\n",
              "  'representations',\n",
              "  'vocabulary',\n",
              "  'number',\n",
              "  'output',\n",
              "  'units',\n",
              "  'need',\n",
              "  'evaluated',\n",
              "  'go',\n",
              "  'around',\n",
              "  'log2'],\n",
              " ['models',\n",
              "  'use',\n",
              "  'hierarchical',\n",
              "  'softmax',\n",
              "  'vocabulary',\n",
              "  'represented',\n",
              "  'huffman',\n",
              "  'binary',\n",
              "  'tree'],\n",
              " ['huffman',\n",
              "  'trees',\n",
              "  'assign',\n",
              "  'short',\n",
              "  'binary',\n",
              "  'codes',\n",
              "  'frequent',\n",
              "  'words',\n",
              "  'reduces',\n",
              "  'number',\n",
              "  'output',\n",
              "  'units',\n",
              "  'need',\n",
              "  'evaluated',\n",
              "  'balanced',\n",
              "  'binary',\n",
              "  'tree',\n",
              "  'would',\n",
              "  'require',\n",
              "  'log2',\n",
              "  'outputs',\n",
              "  'evaluated',\n",
              "  'huffman',\n",
              "  'tree',\n",
              "  'based',\n",
              "  'hierarchical',\n",
              "  'softmax',\n",
              "  'requires',\n",
              "  'log2',\n",
              "  'unigram',\n",
              "  'perplexity'],\n",
              " ['example',\n",
              "  'vocabulary',\n",
              "  'size',\n",
              "  'one',\n",
              "  'million',\n",
              "  'words',\n",
              "  'results',\n",
              "  'two',\n",
              "  'times',\n",
              "  'speedup',\n",
              "  'evaluation'],\n",
              " ['crucial',\n",
              "  'speedup',\n",
              "  'neural',\n",
              "  'network',\n",
              "  'lms',\n",
              "  'computational',\n",
              "  'bottleneck',\n",
              "  'n\\x02d\\x02hterm',\n",
              "  'later',\n",
              "  'propose',\n",
              "  'architectures',\n",
              "  'hidden',\n",
              "  'layers',\n",
              "  'thus',\n",
              "  'depend',\n",
              "  'heavily',\n",
              "  'efﬁciency',\n",
              "  'softmax',\n",
              "  'normalization'],\n",
              " ['rnn', 'model', 'projection', 'layer', 'input', 'hidden', 'output', 'layer'],\n",
              " ['special',\n",
              "  'type',\n",
              "  'model',\n",
              "  'recurrent',\n",
              "  'matrix',\n",
              "  'connects',\n",
              "  'hidden',\n",
              "  'layer',\n",
              "  'using',\n",
              "  'timedelayed',\n",
              "  'connections'],\n",
              " ['allows',\n",
              "  'recurrent',\n",
              "  'model',\n",
              "  'form',\n",
              "  'kind',\n",
              "  'short',\n",
              "  'term',\n",
              "  'memory',\n",
              "  'information',\n",
              "  'past',\n",
              "  'represented',\n",
              "  'hidden',\n",
              "  'layer',\n",
              "  'state',\n",
              "  'gets',\n",
              "  'updated',\n",
              "  'based',\n",
              "  'current',\n",
              "  'input',\n",
              "  'state',\n",
              "  'hidden',\n",
              "  'layer',\n",
              "  'previous',\n",
              "  'time',\n",
              "  'step'],\n",
              " ['framework',\n",
              "  'allows',\n",
              "  'us',\n",
              "  'run',\n",
              "  'multiple',\n",
              "  'replicas',\n",
              "  'model',\n",
              "  'parallel',\n",
              "  'replica',\n",
              "  'synchronizes',\n",
              "  'gradient',\n",
              "  'updates',\n",
              "  'centralized',\n",
              "  'server',\n",
              "  'keeps',\n",
              "  'parameters'],\n",
              " ['framework',\n",
              "  'common',\n",
              "  'use',\n",
              "  'one',\n",
              "  'hundred',\n",
              "  'model',\n",
              "  'replicas',\n",
              "  'using',\n",
              "  'many',\n",
              "  'cpu',\n",
              "  'cores',\n",
              "  'different',\n",
              "  'machines',\n",
              "  'data',\n",
              "  'center'],\n",
              " ['section',\n",
              "  'propose',\n",
              "  'two',\n",
              "  'new',\n",
              "  'model',\n",
              "  'architectures',\n",
              "  'learning',\n",
              "  'distributed',\n",
              "  'representations',\n",
              "  'words',\n",
              "  'try',\n",
              "  'minimize',\n",
              "  'computational',\n",
              "  'complexity'],\n",
              " ['main',\n",
              "  'observation',\n",
              "  'previous',\n",
              "  'section',\n",
              "  'complexity',\n",
              "  'caused',\n",
              "  'nonlinear',\n",
              "  'hidden',\n",
              "  'layer',\n",
              "  'model'],\n",
              " ['makes',\n",
              "  'neural',\n",
              "  'networks',\n",
              "  'attractive',\n",
              "  'decided',\n",
              "  'explore',\n",
              "  'simpler',\n",
              "  'models',\n",
              "  'might',\n",
              "  'able',\n",
              "  'represent',\n",
              "  'data',\n",
              "  'precisely',\n",
              "  'neural',\n",
              "  'networks',\n",
              "  'possibly',\n",
              "  'trained',\n",
              "  'much',\n",
              "  'data',\n",
              "  'efﬁciently'],\n",
              " ['ﬁrst',\n",
              "  'proposed',\n",
              "  'architecture',\n",
              "  'similar',\n",
              "  'feedforward',\n",
              "  'nnlm',\n",
              "  'nonlinear',\n",
              "  'hidden',\n",
              "  'layer',\n",
              "  'removed',\n",
              "  'projection',\n",
              "  'layer',\n",
              "  'shared',\n",
              "  'words',\n",
              "  'projection',\n",
              "  'matrix',\n",
              "  'thus',\n",
              "  'words',\n",
              "  'get',\n",
              "  'projected',\n",
              "  'position',\n",
              "  'vectors',\n",
              "  'averaged'],\n",
              " ['call',\n",
              "  'archi',\n",
              "  'tecture',\n",
              "  'bagofwords',\n",
              "  'model',\n",
              "  'order',\n",
              "  'words',\n",
              "  'history',\n",
              "  'inﬂuence',\n",
              "  'projection'],\n",
              " ['furthermore',\n",
              "  'also',\n",
              "  'use',\n",
              "  'words',\n",
              "  'future',\n",
              "  'obtained',\n",
              "  'best',\n",
              "  'performance',\n",
              "  'task',\n",
              "  'introduced',\n",
              "  'next',\n",
              "  'section',\n",
              "  'building',\n",
              "  'loglinear',\n",
              "  'classiﬁer',\n",
              "  'four',\n",
              "  'future',\n",
              "  'four',\n",
              "  'history',\n",
              "  'words',\n",
              "  'input',\n",
              "  'training',\n",
              "  'criterion',\n",
              "  'correctly',\n",
              "  'classify',\n",
              "  'current',\n",
              "  'middle',\n",
              "  'word'],\n",
              " ['note',\n",
              "  'weight',\n",
              "  'matrix',\n",
              "  'input',\n",
              "  'projection',\n",
              "  'layer',\n",
              "  'shared',\n",
              "  'word',\n",
              "  'positions',\n",
              "  'way',\n",
              "  'nnlm'],\n",
              " ['second',\n",
              "  'architecture',\n",
              "  'similar',\n",
              "  'cbow',\n",
              "  'instead',\n",
              "  'predicting',\n",
              "  'current',\n",
              "  'word',\n",
              "  'based',\n",
              "  'context',\n",
              "  'tries',\n",
              "  'maximize',\n",
              "  'classiﬁcation',\n",
              "  'word',\n",
              "  'based',\n",
              "  'another',\n",
              "  'word',\n",
              "  'sentence'],\n",
              " ['precisely',\n",
              "  'use',\n",
              "  'current',\n",
              "  'word',\n",
              "  'input',\n",
              "  'loglinear',\n",
              "  'classiﬁer',\n",
              "  'continuous',\n",
              "  'projection',\n",
              "  'layer',\n",
              "  'predict',\n",
              "  'words',\n",
              "  'within',\n",
              "  'certain',\n",
              "  'range',\n",
              "  'current',\n",
              "  'word'],\n",
              " ['found',\n",
              "  'increasing',\n",
              "  'range',\n",
              "  'improves',\n",
              "  'quality',\n",
              "  'resulting',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'also',\n",
              "  'increases',\n",
              "  'computational',\n",
              "  'complexity'],\n",
              " ['since',\n",
              "  'distant',\n",
              "  'words',\n",
              "  'usually',\n",
              "  'less',\n",
              "  'related',\n",
              "  'current',\n",
              "  'word',\n",
              "  'close',\n",
              "  'give',\n",
              "  'less',\n",
              "  'weight',\n",
              "  'distant',\n",
              "  'words',\n",
              "  'sampling',\n",
              "  'less',\n",
              "  'words',\n",
              "  'training',\n",
              "  'examples'],\n",
              " ['cbow',\n",
              "  'architecture',\n",
              "  'predicts',\n",
              "  'current',\n",
              "  'word',\n",
              "  'based',\n",
              "  'context',\n",
              "  'skipgram',\n",
              "  'predicts',\n",
              "  'surrounding',\n",
              "  'words',\n",
              "  'given',\n",
              "  'current',\n",
              "  'word'],\n",
              " ['words', 'future', 'current', 'word', 'correct', 'labels'],\n",
              " ['require',\n",
              "  'us',\n",
              "  'r\\x022',\n",
              "  'word',\n",
              "  'classiﬁcations',\n",
              "  'current',\n",
              "  'word',\n",
              "  'input',\n",
              "  'rrwords',\n",
              "  'output'],\n",
              " ['compare',\n",
              "  'quality',\n",
              "  'different',\n",
              "  'versions',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'previous',\n",
              "  'papers',\n",
              "  'typically',\n",
              "  'use',\n",
              "  'table',\n",
              "  'showing',\n",
              "  'example',\n",
              "  'words',\n",
              "  'similar',\n",
              "  'words',\n",
              "  'understand',\n",
              "  'intuitively'],\n",
              " ['although',\n",
              "  'easy',\n",
              "  'show',\n",
              "  'word',\n",
              "  'france',\n",
              "  'similar',\n",
              "  'italy',\n",
              "  'perhaps',\n",
              "  'countries',\n",
              "  'much',\n",
              "  'challenging',\n",
              "  'subjecting',\n",
              "  'vectors',\n",
              "  'complex',\n",
              "  'similarity',\n",
              "  'task',\n",
              "  'follows'],\n",
              " ['follow',\n",
              "  'previous',\n",
              "  'observation',\n",
              "  'many',\n",
              "  'different',\n",
              "  'types',\n",
              "  'similarities',\n",
              "  'words',\n",
              "  'example',\n",
              "  'word',\n",
              "  'bigis',\n",
              "  'similar',\n",
              "  'bigger',\n",
              "  'sense',\n",
              "  'small',\n",
              "  'similar',\n",
              "  'smaller'],\n",
              " ['denote',\n",
              "  'two',\n",
              "  'pairs',\n",
              "  'words',\n",
              "  'relationship',\n",
              "  'question',\n",
              "  'ask',\n",
              "  'word',\n",
              "  'similar',\n",
              "  'small',\n",
              "  'sense',\n",
              "  'biggest',\n",
              "  'similar',\n",
              "  'big'],\n",
              " ['somewhat',\n",
              "  'surprisingly',\n",
              "  'questions',\n",
              "  'answered',\n",
              "  'performing',\n",
              "  'simple',\n",
              "  'algebraic',\n",
              "  'operations',\n",
              "  'vector',\n",
              "  'representation',\n",
              "  'words'],\n",
              " ['search',\n",
              "  'vector',\n",
              "  'space',\n",
              "  'word',\n",
              "  'closest',\n",
              "  'xmeasured',\n",
              "  'cosine',\n",
              "  'distance',\n",
              "  'use',\n",
              "  'answer',\n",
              "  'question',\n",
              "  'discard',\n",
              "  'input',\n",
              "  'question',\n",
              "  'words',\n",
              "  'search'],\n",
              " ['word',\n",
              "  'vectors',\n",
              "  'well',\n",
              "  'trained',\n",
              "  'possible',\n",
              "  'ﬁnd',\n",
              "  'correct',\n",
              "  'answer',\n",
              "  'word',\n",
              "  'smallest',\n",
              "  'using',\n",
              "  'method'],\n",
              " ['finally',\n",
              "  'found',\n",
              "  'train',\n",
              "  'high',\n",
              "  'dimensional',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'large',\n",
              "  'amount',\n",
              "  'data',\n",
              "  'resulting',\n",
              "  'vectors',\n",
              "  'used',\n",
              "  'answer',\n",
              "  'subtle',\n",
              "  'semantic',\n",
              "  'relationships',\n",
              "  'words',\n",
              "  'city',\n",
              "  'country',\n",
              "  'belongs',\n",
              "  'eg',\n",
              "  'france',\n",
              "  'paris',\n",
              "  'germany',\n",
              "  'berlin'],\n",
              " ['word',\n",
              "  'vectors',\n",
              "  'semantic',\n",
              "  'relationships',\n",
              "  'could',\n",
              "  'used',\n",
              "  'improve',\n",
              "  'many',\n",
              "  'existing',\n",
              "  'nlp',\n",
              "  'applications',\n",
              "  'machine',\n",
              "  'translation',\n",
              "  'information',\n",
              "  'retrieval',\n",
              "  'question',\n",
              "  'answering',\n",
              "  'systems',\n",
              "  'may',\n",
              "  'enable',\n",
              "  'future',\n",
              "  'applications',\n",
              "  'yet',\n",
              "  'invented'],\n",
              " ['overall', '8869', 'semantic', '10675', 'syntactic', 'questions'],\n",
              " ['questions',\n",
              "  'category',\n",
              "  'created',\n",
              "  'two',\n",
              "  'steps',\n",
              "  'ﬁrst',\n",
              "  'list',\n",
              "  'similar',\n",
              "  'word',\n",
              "  'pairs',\n",
              "  'created',\n",
              "  'manually'],\n",
              " ['large',\n",
              "  'list',\n",
              "  'questions',\n",
              "  'formed',\n",
              "  'connecting',\n",
              "  'two',\n",
              "  'word',\n",
              "  'pairs'],\n",
              " ['example',\n",
              "  'made',\n",
              "  'list',\n",
              "  '68',\n",
              "  'large',\n",
              "  'american',\n",
              "  'cities',\n",
              "  'states',\n",
              "  'belong',\n",
              "  'formed',\n",
              "  '25k',\n",
              "  'questions',\n",
              "  'picking',\n",
              "  'two',\n",
              "  'word',\n",
              "  'pairs',\n",
              "  'random'],\n",
              " ['included',\n",
              "  'test',\n",
              "  'set',\n",
              "  'single',\n",
              "  'token',\n",
              "  'words',\n",
              "  'thus',\n",
              "  'multiword',\n",
              "  'entities',\n",
              "  'present',\n",
              "  'new',\n",
              "  'york'],\n",
              " ['evaluate',\n",
              "  'overall',\n",
              "  'accuracy',\n",
              "  'question',\n",
              "  'types',\n",
              "  'question',\n",
              "  'type',\n",
              "  'separately',\n",
              "  'se',\n",
              "  'mantic',\n",
              "  'syntactic'],\n",
              " ['also',\n",
              "  'means',\n",
              "  'reaching',\n",
              "  '100',\n",
              "  'accuracy',\n",
              "  'likely',\n",
              "  'impossible',\n",
              "  'current',\n",
              "  'models',\n",
              "  'input',\n",
              "  'information',\n",
              "  'word',\n",
              "  'morphology'],\n",
              " ['however',\n",
              "  'believe',\n",
              "  'usefulness',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'certain',\n",
              "  'applications',\n",
              "  'positively',\n",
              "  'correlated',\n",
              "  'accuracy',\n",
              "  'metric'],\n",
              " ['progress',\n",
              "  'achieved',\n",
              "  'incorporating',\n",
              "  'information',\n",
              "  'structure',\n",
              "  'words',\n",
              "  'especially',\n",
              "  'syntactic',\n",
              "  'questions'],\n",
              " ['used', 'google', 'news', 'corpus', 'training', 'word', 'vectors'],\n",
              " ['restricted', 'vocabulary', 'size', 'million', 'frequent', 'words'],\n",
              " ['clearly',\n",
              "  'facing',\n",
              "  'time',\n",
              "  'constrained',\n",
              "  'optimization',\n",
              "  'problem',\n",
              "  'expected',\n",
              "  'using',\n",
              "  'data',\n",
              "  'higher',\n",
              "  'dimensional',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'improve',\n",
              "  'accuracy'],\n",
              " ['estimate',\n",
              "  'best',\n",
              "  'choice',\n",
              "  'model',\n",
              "  'architecture',\n",
              "  'obtaining',\n",
              "  'good',\n",
              "  'possible',\n",
              "  'results',\n",
              "  'quickly',\n",
              "  'ﬁrst',\n",
              "  'evaluated',\n",
              "  'models',\n",
              "  'trained',\n",
              "  'subsets',\n",
              "  'training',\n",
              "  'data',\n",
              "  'vocabulary',\n",
              "  'restricted',\n",
              "  'frequent',\n",
              "  '30k',\n",
              "  'words'],\n",
              " ['seen',\n",
              "  'point',\n",
              "  'adding',\n",
              "  'dimensions',\n",
              "  'adding',\n",
              "  'training',\n",
              "  'data',\n",
              "  'provides',\n",
              "  'diminishing',\n",
              "  'improvements'],\n",
              " ['increase',\n",
              "  'vector',\n",
              "  'dimensionality',\n",
              "  'amount',\n",
              "  'training',\n",
              "  'data',\n",
              "  'together'],\n",
              " ['questions', 'containing', 'words', 'frequent', '30k', 'words', 'used'],\n",
              " ['given',\n",
              "  'equation',\n",
              "  'increasing',\n",
              "  'amount',\n",
              "  'training',\n",
              "  'data',\n",
              "  'twice',\n",
              "  'results',\n",
              "  'increase',\n",
              "  'computational',\n",
              "  'complexity',\n",
              "  'increasing',\n",
              "  'vector',\n",
              "  'size',\n",
              "  'twice'],\n",
              " ['chose',\n",
              "  'starting',\n",
              "  'learning',\n",
              "  'rate',\n",
              "  '0025',\n",
              "  'decreased',\n",
              "  'linearly',\n",
              "  'approaches',\n",
              "  'zero',\n",
              "  'end',\n",
              "  'last',\n",
              "  'training',\n",
              "  'epoch'],\n",
              " ['first',\n",
              "  'compare',\n",
              "  'different',\n",
              "  'model',\n",
              "  'architectures',\n",
              "  'deriving',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'using',\n",
              "  'training',\n",
              "  'data',\n",
              "  'using',\n",
              "  'dimensionality',\n",
              "  '640',\n",
              "  'word',\n",
              "  'vectors'],\n",
              " ['experiments',\n",
              "  'use',\n",
              "  'full',\n",
              "  'set',\n",
              "  'questions',\n",
              "  'new',\n",
              "  'semanticsyntactic',\n",
              "  'word',\n",
              "  'relationship',\n",
              "  'test',\n",
              "  'set',\n",
              "  'ie',\n",
              "  'unrestricted',\n",
              "  '30k',\n",
              "  'vocabulary'],\n",
              " ['used',\n",
              "  'data',\n",
              "  'provide',\n",
              "  'comparison',\n",
              "  'previously',\n",
              "  'trained',\n",
              "  'recurrent',\n",
              "  'neural',\n",
              "  'network',\n",
              "  'language',\n",
              "  'model',\n",
              "  'took',\n",
              "  'weeks',\n",
              "  'train',\n",
              "  'single',\n",
              "  'cpu'],\n",
              " ['nnlm',\n",
              "  'vectors',\n",
              "  'perform',\n",
              "  'signiﬁcantly',\n",
              "  'better',\n",
              "  'rnn',\n",
              "  'surprising',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'rnnlm',\n",
              "  'directly',\n",
              "  'connected',\n",
              "  'nonlinear',\n",
              "  'hidden',\n",
              "  'layer'],\n",
              " ['cbow',\n",
              "  'architecture',\n",
              "  'works',\n",
              "  'better',\n",
              "  'nnlm',\n",
              "  'syntactic',\n",
              "  'tasks',\n",
              "  'semantic',\n",
              "  'one'],\n",
              " ['finally',\n",
              "  'skipgram',\n",
              "  'architecture',\n",
              "  'works',\n",
              "  'slightly',\n",
              "  'worse',\n",
              "  'syntactic',\n",
              "  'task',\n",
              "  'cbow',\n",
              "  'model',\n",
              "  'still',\n",
              "  'better',\n",
              "  'nnlm',\n",
              "  'much',\n",
              "  'better',\n",
              "  'semantic',\n",
              "  'part',\n",
              "  'test',\n",
              "  'models'],\n",
              " ['next',\n",
              "  'evaluated',\n",
              "  'models',\n",
              "  'trained',\n",
              "  'using',\n",
              "  'one',\n",
              "  'cpu',\n",
              "  'compared',\n",
              "  'results',\n",
              "  'publicly',\n",
              "  'available',\n",
              "  'word',\n",
              "  'vectors'],\n",
              " ['cbow',\n",
              "  'model',\n",
              "  'trained',\n",
              "  'subset',\n",
              "  '3we',\n",
              "  'thank',\n",
              "  'geoff',\n",
              "  'zweig',\n",
              "  'providing',\n",
              "  'us',\n",
              "  'test',\n",
              "  'set'],\n",
              " ['accuracy', 'reported', 'full', 'semanticsyntactic', 'data', 'set'],\n",
              " ['experiments',\n",
              "  'reported',\n",
              "  'used',\n",
              "  'one',\n",
              "  'training',\n",
              "  'epoch',\n",
              "  'decrease',\n",
              "  'learning',\n",
              "  'rate',\n",
              "  'linearly',\n",
              "  'approaches',\n",
              "  'zero',\n",
              "  'end',\n",
              "  'training'],\n",
              " ['mentioned',\n",
              "  'earlier',\n",
              "  'implemented',\n",
              "  'various',\n",
              "  'models',\n",
              "  'distributed',\n",
              "  'framework',\n",
              "  'called',\n",
              "  'dis',\n",
              "  'tbelief'],\n",
              " ['note',\n",
              "  'training',\n",
              "  'nnlm',\n",
              "  '1000dimensional',\n",
              "  'vectors',\n",
              "  'would',\n",
              "  'take',\n",
              "  'long',\n",
              "  'complete'],\n",
              " ['note',\n",
              "  'due',\n",
              "  'overhead',\n",
              "  'distributed',\n",
              "  'framework',\n",
              "  'cpu',\n",
              "  'usage',\n",
              "  'cbow',\n",
              "  'model',\n",
              "  'skipgram',\n",
              "  'model',\n",
              "  'much',\n",
              "  'closer',\n",
              "  'singlemachine',\n",
              "  'implementations'],\n",
              " ['task',\n",
              "  'consists',\n",
              "  '1040',\n",
              "  'sentences',\n",
              "  'one',\n",
              "  'word',\n",
              "  'missing',\n",
              "  'sentence',\n",
              "  'goal',\n",
              "  'select',\n",
              "  'word',\n",
              "  'coherent',\n",
              "  'rest',\n",
              "  'sentence',\n",
              "  'given',\n",
              "  'list',\n",
              "  'ﬁve',\n",
              "  'reasonable',\n",
              "  'choices'],\n",
              " ['explored', 'performance', 'skipgram', 'architecture', 'task'],\n",
              " ['compute',\n",
              "  'score',\n",
              "  'sentence',\n",
              "  'test',\n",
              "  'set',\n",
              "  'using',\n",
              "  'unknown',\n",
              "  'word',\n",
              "  'input',\n",
              "  'predict',\n",
              "  'surrounding',\n",
              "  'words',\n",
              "  'sentence'],\n",
              " ['ﬁnal', 'sentence', 'score', 'sum', 'individual', 'predictions'],\n",
              " ['using', 'sentence', 'scores', 'choose', 'likely', 'sentence'],\n",
              " ['follow',\n",
              "  'approach',\n",
              "  'described',\n",
              "  'relationship',\n",
              "  'deﬁned',\n",
              "  'subtracting',\n",
              "  'two',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'result',\n",
              "  'added',\n",
              "  'another',\n",
              "  'word'],\n",
              " ['believe',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'trained',\n",
              "  'even',\n",
              "  'larger',\n",
              "  'data',\n",
              "  'sets',\n",
              "  'larger',\n",
              "  'dimensionality',\n",
              "  'perform',\n",
              "  'signiﬁcantly',\n",
              "  'better',\n",
              "  'enable',\n",
              "  'development',\n",
              "  'new',\n",
              "  'innovative',\n",
              "  'applications'],\n",
              " ['another',\n",
              "  'way',\n",
              "  'improve',\n",
              "  'accuracy',\n",
              "  'provide',\n",
              "  'one',\n",
              "  'example',\n",
              "  'relationship'],\n",
              " ['using',\n",
              "  'ten',\n",
              "  'examples',\n",
              "  'instead',\n",
              "  'one',\n",
              "  'form',\n",
              "  'relationship',\n",
              "  'vector',\n",
              "  'average',\n",
              "  'individual',\n",
              "  'vectors',\n",
              "  'together',\n",
              "  'observed',\n",
              "  'improvement',\n",
              "  'accuracy',\n",
              "  'best',\n",
              "  'models',\n",
              "  '10',\n",
              "  'absolutely',\n",
              "  'semanticsyntactic',\n",
              "  'test'],\n",
              " ['also',\n",
              "  'possible',\n",
              "  'apply',\n",
              "  'vector',\n",
              "  'operations',\n",
              "  'solve',\n",
              "  'different',\n",
              "  'tasks'],\n",
              " ['example',\n",
              "  'observed',\n",
              "  'good',\n",
              "  'accuracy',\n",
              "  'selecting',\n",
              "  'outofthelist',\n",
              "  'words',\n",
              "  'computing',\n",
              "  'average',\n",
              "  'vector',\n",
              "  'list',\n",
              "  'words',\n",
              "  'ﬁnding',\n",
              "  'distant',\n",
              "  'word',\n",
              "  'vector'],\n",
              " ['popular', 'type', 'problems', 'certain', 'human', 'intelligence', 'tests'],\n",
              " ['clearly', 'still', 'lot', 'discoveries', 'made', 'using', 'techniques'],\n",
              " ['paper',\n",
              "  'studied',\n",
              "  'quality',\n",
              "  'vector',\n",
              "  'representations',\n",
              "  'words',\n",
              "  'derived',\n",
              "  'various',\n",
              "  'models',\n",
              "  'collection',\n",
              "  'syntactic',\n",
              "  'semantic',\n",
              "  'language',\n",
              "  'tasks'],\n",
              " ['observed',\n",
              "  'possible',\n",
              "  'train',\n",
              "  'high',\n",
              "  'quality',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'using',\n",
              "  'simple',\n",
              "  'model',\n",
              "  'architectures',\n",
              "  'compared',\n",
              "  'popular',\n",
              "  'neural',\n",
              "  'network',\n",
              "  'models',\n",
              "  'feedforward',\n",
              "  'recurrent'],\n",
              " ['much',\n",
              "  'lower',\n",
              "  'computational',\n",
              "  'complexity',\n",
              "  'possible',\n",
              "  'compute',\n",
              "  'accurate',\n",
              "  'high',\n",
              "  'dimensional',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'much',\n",
              "  'larger',\n",
              "  'data',\n",
              "  'set'],\n",
              " ['using',\n",
              "  'distbelief',\n",
              "  'distributed',\n",
              "  'framework',\n",
              "  'possible',\n",
              "  'train',\n",
              "  'cbow',\n",
              "  'skipgram',\n",
              "  'models',\n",
              "  'even',\n",
              "  'corpora',\n",
              "  'one',\n",
              "  'trillion',\n",
              "  'words',\n",
              "  'basically',\n",
              "  'unlimited',\n",
              "  'size',\n",
              "  'vocabulary'],\n",
              " ['several',\n",
              "  'orders',\n",
              "  'magnitude',\n",
              "  'larger',\n",
              "  'best',\n",
              "  'previously',\n",
              "  'published',\n",
              "  'results',\n",
              "  'similar',\n",
              "  'models'],\n",
              " ['expected',\n",
              "  'applications',\n",
              "  'beneﬁt',\n",
              "  'model',\n",
              "  'architectures',\n",
              "  'described',\n",
              "  'paper'],\n",
              " ['ongoing',\n",
              "  'work',\n",
              "  'shows',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'successfully',\n",
              "  'applied',\n",
              "  'automatic',\n",
              "  'extension',\n",
              "  'facts',\n",
              "  'knowledge',\n",
              "  'bases',\n",
              "  'also',\n",
              "  'veriﬁcation',\n",
              "  'correctness',\n",
              "  'existing',\n",
              "  'facts'],\n",
              " ['results',\n",
              "  'machine',\n",
              "  'translation',\n",
              "  'experiments',\n",
              "  'also',\n",
              "  'look',\n",
              "  'promising'],\n",
              " ['believe',\n",
              "  'comprehensive',\n",
              "  'test',\n",
              "  'set',\n",
              "  'help',\n",
              "  'research',\n",
              "  'community',\n",
              "  'improve',\n",
              "  'existing',\n",
              "  'techniques',\n",
              "  'estimating',\n",
              "  'word',\n",
              "  'vectors'],\n",
              " ['also',\n",
              "  'expect',\n",
              "  'high',\n",
              "  'quality',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'become',\n",
              "  'important',\n",
              "  'building',\n",
              "  'block',\n",
              "  'future',\n",
              "  'nlp',\n",
              "  'applications'],\n",
              " ['initial',\n",
              "  'version',\n",
              "  'paper',\n",
              "  'written',\n",
              "  'published',\n",
              "  'singlemachine',\n",
              "  'multithreaded',\n",
              "  'code',\n",
              "  'computing',\n",
              "  'word',\n",
              "  'vectors',\n",
              "  'using',\n",
              "  'continuous',\n",
              "  'bagofwords',\n",
              "  'skipgram',\n",
              "  'archi',\n",
              "  'tectures4'],\n",
              " ['training',\n",
              "  'speed',\n",
              "  'signiﬁcantly',\n",
              "  'higher',\n",
              "  'reported',\n",
              "  'earlier',\n",
              "  'paper',\n",
              "  'ie',\n",
              "  'order',\n",
              "  'billions',\n",
              "  'words',\n",
              "  'per',\n",
              "  'hour',\n",
              "  'typical',\n",
              "  'hyperparameter',\n",
              "  'choices'],\n",
              " ['also',\n",
              "  'published',\n",
              "  '14',\n",
              "  'million',\n",
              "  'vectors',\n",
              "  'represent',\n",
              "  'named',\n",
              "  'entities',\n",
              "  'trained',\n",
              "  '100',\n",
              "  'billion',\n",
              "  'words'],\n",
              " ['proceedings',\n",
              "  'joint',\n",
              "  'conference',\n",
              "  'empirical',\n",
              "  'methods',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'computational',\n",
              "  'language',\n",
              "  'learning',\n",
              "  '2007'],\n",
              " ['uniﬁed',\n",
              "  'architecture',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'deep',\n",
              "  'neural',\n",
              "  'networks',\n",
              "  'multitask',\n",
              "  'learning'],\n",
              " ['international', 'conference', 'machine', 'learning', 'icml', '2008'],\n",
              " ['adaptive',\n",
              "  'subgradient',\n",
              "  'methods',\n",
              "  'online',\n",
              "  'learning',\n",
              "  'stochastic',\n",
              "  'optimization'],\n",
              " ['improving',\n",
              "  'word',\n",
              "  'representations',\n",
              "  'via',\n",
              "  'global',\n",
              "  'context',\n",
              "  'multiple',\n",
              "  'word',\n",
              "  'prototypes'],\n",
              " ['parallel',\n",
              "  'dis',\n",
              "  'tributed',\n",
              "  'processing',\n",
              "  'explorations',\n",
              "  'microstructure',\n",
              "  'cognition'],\n",
              " ['semeval2012', 'task', 'measuring', 'degrees', 'relational', 'similarity'],\n",
              " ['language',\n",
              "  'modeling',\n",
              "  'speech',\n",
              "  'recognition',\n",
              "  'czech',\n",
              "  'masters',\n",
              "  'thesis',\n",
              "  'brno',\n",
              "  'uni',\n",
              "  'versity',\n",
              "  'technology',\n",
              "  '2007'],\n",
              " ['neural',\n",
              "  'network',\n",
              "  'based',\n",
              "  'lan',\n",
              "  'guage',\n",
              "  'models',\n",
              "  'higly',\n",
              "  'inﬂective',\n",
              "  'languages',\n",
              "  'proc'],\n",
              " ['recurrent',\n",
              "  'neural',\n",
              "  'network',\n",
              "  'based',\n",
              "  'language',\n",
              "  'model',\n",
              "  'proceedings',\n",
              "  'interspeech',\n",
              "  '2010'],\n",
              " ['extensions',\n",
              "  'recurrent',\n",
              "  'neural',\n",
              "  'network',\n",
              "  'language',\n",
              "  'model',\n",
              "  'proceedings',\n",
              "  'icassp',\n",
              "  '2011'],\n",
              " ['empirical',\n",
              "  'evaluation',\n",
              "  'com',\n",
              "  'bination',\n",
              "  'advanced',\n",
              "  'language',\n",
              "  'modeling',\n",
              "  'techniques',\n",
              "  'proceedings',\n",
              "  'interspeech',\n",
              "  '2011'],\n",
              " ['linguistic',\n",
              "  'regularities',\n",
              "  'continuous',\n",
              "  'space',\n",
              "  'word',\n",
              "  'represen',\n",
              "  'tations'],\n",
              " ['distributed', 'representations', 'words', 'phrases', 'compositionality'],\n",
              " ['three',\n",
              "  'new',\n",
              "  'graphical',\n",
              "  'models',\n",
              "  'statistical',\n",
              "  'language',\n",
              "  'modelling'],\n",
              " ['advances',\n",
              "  'neural',\n",
              "  'information',\n",
              "  'processing',\n",
              "  'systems',\n",
              "  '21',\n",
              "  'mit',\n",
              "  'press',\n",
              "  '2009'],\n",
              " ['fast',\n",
              "  'simple',\n",
              "  'algorithm',\n",
              "  'training',\n",
              "  'neural',\n",
              "  'probabilistic',\n",
              "  'language',\n",
              "  'models'],\n",
              " ['learning', 'internal', 'representations', 'back', 'propagating', 'errors'],\n",
              " ['dynamic',\n",
              "  'pooling',\n",
              "  'unfolding',\n",
              "  'recursive',\n",
              "  'autoencoders',\n",
              "  'paraphrase',\n",
              "  'detection'],\n",
              " ['word',\n",
              "  'representations',\n",
              "  'simple',\n",
              "  'general',\n",
              "  'method',\n",
              "  'semisupervised',\n",
              "  'learning'],\n",
              " ['measuring', 'semantic', 'similarity', 'latent', 'relational', 'analysis'],\n",
              " ['interna',\n",
              "  'tional',\n",
              "  'joint',\n",
              "  'conference',\n",
              "  'artiﬁcial',\n",
              "  'intelligence',\n",
              "  '2005'],\n",
              " ['combining',\n",
              "  'heterogeneous',\n",
              "  'models',\n",
              "  'measuring',\n",
              "  'relational',\n",
              "  'similarity']]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Course/AIR/data/w2vdata/homework2_dataset.csv')\n",
        "df['txt'] = df['txt'].apply(ast.literal_eval)\n",
        "docs = [col for col in df['txt']]\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBYJeB2sAPLh",
        "outputId": "d33651b7-0a3c-44e5-c901-300bb44a711a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'quality',\n",
              " 1: 'representations',\n",
              " 2: 'measured',\n",
              " 3: 'word',\n",
              " 4: 'similarity',\n",
              " 5: 'task',\n",
              " 6: 'results',\n",
              " 7: 'compared',\n",
              " 8: 'previously',\n",
              " 9: 'best',\n",
              " 10: 'performing',\n",
              " 11: 'techniques',\n",
              " 12: 'based',\n",
              " 13: 'different',\n",
              " 14: 'types',\n",
              " 15: 'neural',\n",
              " 16: 'networks',\n",
              " 17: 'observe',\n",
              " 18: 'large',\n",
              " 19: 'improvements',\n",
              " 20: 'accuracy',\n",
              " 21: 'much',\n",
              " 22: 'lower',\n",
              " 23: 'computational',\n",
              " 24: 'cost',\n",
              " 25: 'ie',\n",
              " 26: 'takes',\n",
              " 27: 'less',\n",
              " 28: 'day',\n",
              " 29: 'learn',\n",
              " 30: 'high',\n",
              " 31: 'vectors',\n",
              " 32: '16',\n",
              " 33: 'billion',\n",
              " 34: 'words',\n",
              " 35: 'data',\n",
              " 36: 'set',\n",
              " 37: 'furthermore',\n",
              " 38: 'show',\n",
              " 39: 'provide',\n",
              " 40: 'stateoftheart',\n",
              " 41: 'performance',\n",
              " 42: 'test',\n",
              " 43: 'measuring',\n",
              " 44: 'syntactic',\n",
              " 45: 'semantic',\n",
              " 46: 'similarities',\n",
              " 47: 'introduction',\n",
              " 48: 'many',\n",
              " 49: 'current',\n",
              " 50: 'nlp',\n",
              " 51: 'systems',\n",
              " 52: 'treat',\n",
              " 53: 'atomic',\n",
              " 54: 'units',\n",
              " 55: 'notion',\n",
              " 56: 'represented',\n",
              " 57: 'indices',\n",
              " 58: 'vocabulary',\n",
              " 59: 'choice',\n",
              " 60: 'several',\n",
              " 61: 'good',\n",
              " 62: 'reasons',\n",
              " 63: 'simplicity',\n",
              " 64: 'robustness',\n",
              " 65: 'observation',\n",
              " 66: 'simple',\n",
              " 67: 'models',\n",
              " 68: 'trained',\n",
              " 69: 'huge',\n",
              " 70: 'amounts',\n",
              " 71: 'outperform',\n",
              " 72: 'complex',\n",
              " 73: 'however',\n",
              " 74: 'limits',\n",
              " 75: 'tasks',\n",
              " 76: 'example',\n",
              " 77: 'amount',\n",
              " 78: 'relevant',\n",
              " 79: 'indomain',\n",
              " 80: 'automatic',\n",
              " 81: 'speech',\n",
              " 82: 'recognition',\n",
              " 83: 'limited',\n",
              " 84: 'usually',\n",
              " 85: 'dominated',\n",
              " 86: 'size',\n",
              " 87: 'transcribed',\n",
              " 88: 'often',\n",
              " 89: 'millions',\n",
              " 90: 'machine',\n",
              " 91: 'translation',\n",
              " 92: 'existing',\n",
              " 93: 'corpora',\n",
              " 94: 'languages',\n",
              " 95: 'contain',\n",
              " 96: 'billions',\n",
              " 97: 'thus',\n",
              " 98: 'situations',\n",
              " 99: 'scaling',\n",
              " 100: 'basic',\n",
              " 101: 'result',\n",
              " 102: 'signiﬁcant',\n",
              " 103: 'progress',\n",
              " 104: 'focus',\n",
              " 105: 'advanced',\n",
              " 106: 'learning',\n",
              " 107: 'recent',\n",
              " 108: 'years',\n",
              " 109: 'become',\n",
              " 110: 'possible',\n",
              " 111: 'train',\n",
              " 112: 'larger',\n",
              " 113: 'typically',\n",
              " 114: 'main',\n",
              " 115: 'goal',\n",
              " 116: 'paper',\n",
              " 117: 'introduce',\n",
              " 118: 'used',\n",
              " 119: 'highquality',\n",
              " 120: 'sets',\n",
              " 121: 'somewhat',\n",
              " 122: 'surprisingly',\n",
              " 123: 'found',\n",
              " 124: 'goes',\n",
              " 125: 'beyond',\n",
              " 126: 'regularities',\n",
              " 127: 'try',\n",
              " 128: 'maximize',\n",
              " 129: 'vector',\n",
              " 130: 'operations',\n",
              " 131: 'developing',\n",
              " 132: 'new',\n",
              " 133: 'model',\n",
              " 134: 'architectures',\n",
              " 135: 'preserve',\n",
              " 136: 'linear',\n",
              " 137: 'among',\n",
              " 138: 'design',\n",
              " 139: 'comprehensive',\n",
              " 140: 'learned',\n",
              " 141: 'moreover',\n",
              " 142: 'discuss',\n",
              " 143: 'training',\n",
              " 144: 'time',\n",
              " 145: 'depends',\n",
              " 146: 'dimensionality',\n",
              " 147: 'even',\n",
              " 148: 'without',\n",
              " 149: 'constructing',\n",
              " 150: 'full',\n",
              " 151: 'nnlm',\n",
              " 152: 'work',\n",
              " 153: 'directly',\n",
              " 154: 'extend',\n",
              " 155: 'architecture',\n",
              " 156: 'ﬁrst',\n",
              " 157: 'step',\n",
              " 158: 'using',\n",
              " 159: 'proposed',\n",
              " 160: 'estimating',\n",
              " 161: 'continuous',\n",
              " 162: 'including',\n",
              " 163: 'wellknown',\n",
              " 164: 'latent',\n",
              " 165: 'analysis',\n",
              " 166: 'lsa',\n",
              " 167: 'dirichlet',\n",
              " 168: 'allocation',\n",
              " 169: 'lda',\n",
              " 170: 'next',\n",
              " 171: 'minimizing',\n",
              " 172: 'complexity',\n",
              " 173: 'input',\n",
              " 174: 'layer',\n",
              " 175: 'previous',\n",
              " 176: 'encoded',\n",
              " 177: '1of',\n",
              " 178: 'vcoding',\n",
              " 179: 'projected',\n",
              " 180: 'projection',\n",
              " 181: 'pthat',\n",
              " 182: 'n\\x02d',\n",
              " 183: 'shared',\n",
              " 184: 'matrix',\n",
              " 185: 'inputs',\n",
              " 186: 'active',\n",
              " 187: 'given',\n",
              " 188: 'composition',\n",
              " 189: 'relatively',\n",
              " 190: 'cheap',\n",
              " 191: 'operation',\n",
              " 192: 'becomes',\n",
              " 193: 'computation',\n",
              " 194: 'hidden',\n",
              " 195: 'values',\n",
              " 196: 'dense',\n",
              " 197: 'binary',\n",
              " 198: 'tree',\n",
              " 199: 'number',\n",
              " 200: 'output',\n",
              " 201: 'need',\n",
              " 202: 'evaluated',\n",
              " 203: 'go',\n",
              " 204: 'around',\n",
              " 205: 'log2',\n",
              " 206: 'use',\n",
              " 207: 'hierarchical',\n",
              " 208: 'softmax',\n",
              " 209: 'huffman',\n",
              " 210: 'trees',\n",
              " 211: 'assign',\n",
              " 212: 'short',\n",
              " 213: 'codes',\n",
              " 214: 'frequent',\n",
              " 215: 'reduces',\n",
              " 216: 'balanced',\n",
              " 217: 'would',\n",
              " 218: 'require',\n",
              " 219: 'outputs',\n",
              " 220: 'requires',\n",
              " 221: 'unigram',\n",
              " 222: 'perplexity',\n",
              " 223: 'one',\n",
              " 224: 'million',\n",
              " 225: 'two',\n",
              " 226: 'times',\n",
              " 227: 'speedup',\n",
              " 228: 'evaluation',\n",
              " 229: 'crucial',\n",
              " 230: 'network',\n",
              " 231: 'lms',\n",
              " 232: 'bottleneck',\n",
              " 233: 'n\\x02d\\x02hterm',\n",
              " 234: 'later',\n",
              " 235: 'propose',\n",
              " 236: 'layers',\n",
              " 237: 'depend',\n",
              " 238: 'heavily',\n",
              " 239: 'efﬁciency',\n",
              " 240: 'normalization',\n",
              " 241: 'rnn',\n",
              " 242: 'special',\n",
              " 243: 'type',\n",
              " 244: 'recurrent',\n",
              " 245: 'connects',\n",
              " 246: 'timedelayed',\n",
              " 247: 'connections',\n",
              " 248: 'allows',\n",
              " 249: 'form',\n",
              " 250: 'kind',\n",
              " 251: 'term',\n",
              " 252: 'memory',\n",
              " 253: 'information',\n",
              " 254: 'past',\n",
              " 255: 'state',\n",
              " 256: 'gets',\n",
              " 257: 'updated',\n",
              " 258: 'framework',\n",
              " 259: 'us',\n",
              " 260: 'run',\n",
              " 261: 'multiple',\n",
              " 262: 'replicas',\n",
              " 263: 'parallel',\n",
              " 264: 'replica',\n",
              " 265: 'synchronizes',\n",
              " 266: 'gradient',\n",
              " 267: 'updates',\n",
              " 268: 'centralized',\n",
              " 269: 'server',\n",
              " 270: 'keeps',\n",
              " 271: 'parameters',\n",
              " 272: 'common',\n",
              " 273: 'hundred',\n",
              " 274: 'cpu',\n",
              " 275: 'cores',\n",
              " 276: 'machines',\n",
              " 277: 'center',\n",
              " 278: 'section',\n",
              " 279: 'distributed',\n",
              " 280: 'minimize',\n",
              " 281: 'caused',\n",
              " 282: 'nonlinear',\n",
              " 283: 'makes',\n",
              " 284: 'attractive',\n",
              " 285: 'decided',\n",
              " 286: 'explore',\n",
              " 287: 'simpler',\n",
              " 288: 'might',\n",
              " 289: 'able',\n",
              " 290: 'represent',\n",
              " 291: 'precisely',\n",
              " 292: 'possibly',\n",
              " 293: 'efﬁciently',\n",
              " 294: 'similar',\n",
              " 295: 'feedforward',\n",
              " 296: 'removed',\n",
              " 297: 'get',\n",
              " 298: 'position',\n",
              " 299: 'averaged',\n",
              " 300: 'call',\n",
              " 301: 'archi',\n",
              " 302: 'tecture',\n",
              " 303: 'bagofwords',\n",
              " 304: 'order',\n",
              " 305: 'history',\n",
              " 306: 'inﬂuence',\n",
              " 307: 'also',\n",
              " 308: 'future',\n",
              " 309: 'obtained',\n",
              " 310: 'introduced',\n",
              " 311: 'building',\n",
              " 312: 'loglinear',\n",
              " 313: 'classiﬁer',\n",
              " 314: 'four',\n",
              " 315: 'criterion',\n",
              " 316: 'correctly',\n",
              " 317: 'classify',\n",
              " 318: 'middle',\n",
              " 319: 'note',\n",
              " 320: 'weight',\n",
              " 321: 'positions',\n",
              " 322: 'way',\n",
              " 323: 'second',\n",
              " 324: 'cbow',\n",
              " 325: 'instead',\n",
              " 326: 'predicting',\n",
              " 327: 'context',\n",
              " 328: 'tries',\n",
              " 329: 'classiﬁcation',\n",
              " 330: 'another',\n",
              " 331: 'sentence',\n",
              " 332: 'predict',\n",
              " 333: 'within',\n",
              " 334: 'certain',\n",
              " 335: 'range',\n",
              " 336: 'increasing',\n",
              " 337: 'improves',\n",
              " 338: 'resulting',\n",
              " 339: 'increases',\n",
              " 340: 'since',\n",
              " 341: 'distant',\n",
              " 342: 'related',\n",
              " 343: 'close',\n",
              " 344: 'give',\n",
              " 345: 'sampling',\n",
              " 346: 'examples',\n",
              " 347: 'predicts',\n",
              " 348: 'skipgram',\n",
              " 349: 'surrounding',\n",
              " 350: 'correct',\n",
              " 351: 'labels',\n",
              " 352: 'r\\x022',\n",
              " 353: 'classiﬁcations',\n",
              " 354: 'rrwords',\n",
              " 355: 'compare',\n",
              " 356: 'versions',\n",
              " 357: 'papers',\n",
              " 358: 'table',\n",
              " 359: 'showing',\n",
              " 360: 'understand',\n",
              " 361: 'intuitively',\n",
              " 362: 'although',\n",
              " 363: 'easy',\n",
              " 364: 'france',\n",
              " 365: 'italy',\n",
              " 366: 'perhaps',\n",
              " 367: 'countries',\n",
              " 368: 'challenging',\n",
              " 369: 'subjecting',\n",
              " 370: 'follows',\n",
              " 371: 'follow',\n",
              " 372: 'bigis',\n",
              " 373: 'bigger',\n",
              " 374: 'sense',\n",
              " 375: 'small',\n",
              " 376: 'smaller',\n",
              " 377: 'denote',\n",
              " 378: 'pairs',\n",
              " 379: 'relationship',\n",
              " 380: 'question',\n",
              " 381: 'ask',\n",
              " 382: 'biggest',\n",
              " 383: 'big',\n",
              " 384: 'questions',\n",
              " 385: 'answered',\n",
              " 386: 'algebraic',\n",
              " 387: 'representation',\n",
              " 388: 'search',\n",
              " 389: 'space',\n",
              " 390: 'closest',\n",
              " 391: 'xmeasured',\n",
              " 392: 'cosine',\n",
              " 393: 'distance',\n",
              " 394: 'answer',\n",
              " 395: 'discard',\n",
              " 396: 'well',\n",
              " 397: 'ﬁnd',\n",
              " 398: 'smallest',\n",
              " 399: 'method',\n",
              " 400: 'finally',\n",
              " 401: 'dimensional',\n",
              " 402: 'subtle',\n",
              " 403: 'relationships',\n",
              " 404: 'city',\n",
              " 405: 'country',\n",
              " 406: 'belongs',\n",
              " 407: 'eg',\n",
              " 408: 'paris',\n",
              " 409: 'germany',\n",
              " 410: 'berlin',\n",
              " 411: 'could',\n",
              " 412: 'improve',\n",
              " 413: 'applications',\n",
              " 414: 'retrieval',\n",
              " 415: 'answering',\n",
              " 416: 'may',\n",
              " 417: 'enable',\n",
              " 418: 'yet',\n",
              " 419: 'invented',\n",
              " 420: 'overall',\n",
              " 421: '8869',\n",
              " 422: '10675',\n",
              " 423: 'category',\n",
              " 424: 'created',\n",
              " 425: 'steps',\n",
              " 426: 'list',\n",
              " 427: 'manually',\n",
              " 428: 'formed',\n",
              " 429: 'connecting',\n",
              " 430: 'made',\n",
              " 431: '68',\n",
              " 432: 'american',\n",
              " 433: 'cities',\n",
              " 434: 'states',\n",
              " 435: 'belong',\n",
              " 436: '25k',\n",
              " 437: 'picking',\n",
              " 438: 'random',\n",
              " 439: 'included',\n",
              " 440: 'single',\n",
              " 441: 'token',\n",
              " 442: 'multiword',\n",
              " 443: 'entities',\n",
              " 444: 'present',\n",
              " 445: 'york',\n",
              " 446: 'evaluate',\n",
              " 447: 'separately',\n",
              " 448: 'se',\n",
              " 449: 'mantic',\n",
              " 450: 'means',\n",
              " 451: 'reaching',\n",
              " 452: '100',\n",
              " 453: 'likely',\n",
              " 454: 'impossible',\n",
              " 455: 'morphology',\n",
              " 456: 'believe',\n",
              " 457: 'usefulness',\n",
              " 458: 'positively',\n",
              " 459: 'correlated',\n",
              " 460: 'metric',\n",
              " 461: 'achieved',\n",
              " 462: 'incorporating',\n",
              " 463: 'structure',\n",
              " 464: 'especially',\n",
              " 465: 'google',\n",
              " 466: 'news',\n",
              " 467: 'corpus',\n",
              " 468: 'restricted',\n",
              " 469: 'clearly',\n",
              " 470: 'facing',\n",
              " 471: 'constrained',\n",
              " 472: 'optimization',\n",
              " 473: 'problem',\n",
              " 474: 'expected',\n",
              " 475: 'higher',\n",
              " 476: 'estimate',\n",
              " 477: 'obtaining',\n",
              " 478: 'quickly',\n",
              " 479: 'subsets',\n",
              " 480: '30k',\n",
              " 481: 'seen',\n",
              " 482: 'point',\n",
              " 483: 'adding',\n",
              " 484: 'dimensions',\n",
              " 485: 'provides',\n",
              " 486: 'diminishing',\n",
              " 487: 'increase',\n",
              " 488: 'together',\n",
              " 489: 'containing',\n",
              " 490: 'equation',\n",
              " 491: 'twice',\n",
              " 492: 'chose',\n",
              " 493: 'starting',\n",
              " 494: 'rate',\n",
              " 495: '0025',\n",
              " 496: 'decreased',\n",
              " 497: 'linearly',\n",
              " 498: 'approaches',\n",
              " 499: 'zero',\n",
              " 500: 'end',\n",
              " 501: 'last',\n",
              " 502: 'epoch',\n",
              " 503: 'first',\n",
              " 504: 'deriving',\n",
              " 505: '640',\n",
              " 506: 'experiments',\n",
              " 507: 'semanticsyntactic',\n",
              " 508: 'unrestricted',\n",
              " 509: 'comparison',\n",
              " 510: 'language',\n",
              " 511: 'took',\n",
              " 512: 'weeks',\n",
              " 513: 'perform',\n",
              " 514: 'signiﬁcantly',\n",
              " 515: 'better',\n",
              " 516: 'surprising',\n",
              " 517: 'rnnlm',\n",
              " 518: 'connected',\n",
              " 519: 'works',\n",
              " 520: 'slightly',\n",
              " 521: 'worse',\n",
              " 522: 'still',\n",
              " 523: 'part',\n",
              " 524: 'publicly',\n",
              " 525: 'available',\n",
              " 526: 'subset',\n",
              " 527: '3we',\n",
              " 528: 'thank',\n",
              " 529: 'geoff',\n",
              " 530: 'zweig',\n",
              " 531: 'providing',\n",
              " 532: 'reported',\n",
              " 533: 'decrease',\n",
              " 534: 'mentioned',\n",
              " 535: 'earlier',\n",
              " 536: 'implemented',\n",
              " 537: 'various',\n",
              " 538: 'called',\n",
              " 539: 'dis',\n",
              " 540: 'tbelief',\n",
              " 541: '1000dimensional',\n",
              " 542: 'take',\n",
              " 543: 'long',\n",
              " 544: 'complete',\n",
              " 545: 'due',\n",
              " 546: 'overhead',\n",
              " 547: 'usage',\n",
              " 548: 'closer',\n",
              " 549: 'singlemachine',\n",
              " 550: 'implementations',\n",
              " 551: 'consists',\n",
              " 552: '1040',\n",
              " 553: 'sentences',\n",
              " 554: 'missing',\n",
              " 555: 'select',\n",
              " 556: 'coherent',\n",
              " 557: 'rest',\n",
              " 558: 'ﬁve',\n",
              " 559: 'reasonable',\n",
              " 560: 'choices',\n",
              " 561: 'explored',\n",
              " 562: 'compute',\n",
              " 563: 'score',\n",
              " 564: 'unknown',\n",
              " 565: 'ﬁnal',\n",
              " 566: 'sum',\n",
              " 567: 'individual',\n",
              " 568: 'predictions',\n",
              " 569: 'scores',\n",
              " 570: 'choose',\n",
              " 571: 'approach',\n",
              " 572: 'described',\n",
              " 573: 'deﬁned',\n",
              " 574: 'subtracting',\n",
              " 575: 'added',\n",
              " 576: 'development',\n",
              " 577: 'innovative',\n",
              " 578: 'ten',\n",
              " 579: 'average',\n",
              " 580: 'observed',\n",
              " 581: 'improvement',\n",
              " 582: '10',\n",
              " 583: 'absolutely',\n",
              " 584: 'apply',\n",
              " 585: 'solve',\n",
              " 586: 'selecting',\n",
              " 587: 'outofthelist',\n",
              " 588: 'computing',\n",
              " 589: 'ﬁnding',\n",
              " 590: 'popular',\n",
              " 591: 'problems',\n",
              " 592: 'human',\n",
              " 593: 'intelligence',\n",
              " 594: 'tests',\n",
              " 595: 'lot',\n",
              " 596: 'discoveries',\n",
              " 597: 'studied',\n",
              " 598: 'derived',\n",
              " 599: 'collection',\n",
              " 600: 'accurate',\n",
              " 601: 'distbelief',\n",
              " 602: 'trillion',\n",
              " 603: 'basically',\n",
              " 604: 'unlimited',\n",
              " 605: 'orders',\n",
              " 606: 'magnitude',\n",
              " 607: 'published',\n",
              " 608: 'beneﬁt',\n",
              " 609: 'ongoing',\n",
              " 610: 'shows',\n",
              " 611: 'successfully',\n",
              " 612: 'applied',\n",
              " 613: 'extension',\n",
              " 614: 'facts',\n",
              " 615: 'knowledge',\n",
              " 616: 'bases',\n",
              " 617: 'veriﬁcation',\n",
              " 618: 'correctness',\n",
              " 619: 'look',\n",
              " 620: 'promising',\n",
              " 621: 'help',\n",
              " 622: 'research',\n",
              " 623: 'community',\n",
              " 624: 'expect',\n",
              " 625: 'important',\n",
              " 626: 'block',\n",
              " 627: 'initial',\n",
              " 628: 'version',\n",
              " 629: 'written',\n",
              " 630: 'multithreaded',\n",
              " 631: 'code',\n",
              " 632: 'tectures4',\n",
              " 633: 'speed',\n",
              " 634: 'per',\n",
              " 635: 'hour',\n",
              " 636: 'typical',\n",
              " 637: 'hyperparameter',\n",
              " 638: '14',\n",
              " 639: 'named',\n",
              " 640: 'proceedings',\n",
              " 641: 'joint',\n",
              " 642: 'conference',\n",
              " 643: 'empirical',\n",
              " 644: 'methods',\n",
              " 645: 'natural',\n",
              " 646: 'processing',\n",
              " 647: '2007',\n",
              " 648: 'uniﬁed',\n",
              " 649: 'deep',\n",
              " 650: 'multitask',\n",
              " 651: 'international',\n",
              " 652: 'icml',\n",
              " 653: '2008',\n",
              " 654: 'adaptive',\n",
              " 655: 'subgradient',\n",
              " 656: 'online',\n",
              " 657: 'stochastic',\n",
              " 658: 'improving',\n",
              " 659: 'via',\n",
              " 660: 'global',\n",
              " 661: 'prototypes',\n",
              " 662: 'tributed',\n",
              " 663: 'explorations',\n",
              " 664: 'microstructure',\n",
              " 665: 'cognition',\n",
              " 666: 'semeval2012',\n",
              " 667: 'degrees',\n",
              " 668: 'relational',\n",
              " 669: 'modeling',\n",
              " 670: 'czech',\n",
              " 671: 'masters',\n",
              " 672: 'thesis',\n",
              " 673: 'brno',\n",
              " 674: 'uni',\n",
              " 675: 'versity',\n",
              " 676: 'technology',\n",
              " 677: 'lan',\n",
              " 678: 'guage',\n",
              " 679: 'higly',\n",
              " 680: 'inﬂective',\n",
              " 681: 'proc',\n",
              " 682: 'interspeech',\n",
              " 683: '2010',\n",
              " 684: 'extensions',\n",
              " 685: 'icassp',\n",
              " 686: '2011',\n",
              " 687: 'com',\n",
              " 688: 'bination',\n",
              " 689: 'linguistic',\n",
              " 690: 'represen',\n",
              " 691: 'tations',\n",
              " 692: 'phrases',\n",
              " 693: 'compositionality',\n",
              " 694: 'three',\n",
              " 695: 'graphical',\n",
              " 696: 'statistical',\n",
              " 697: 'modelling',\n",
              " 698: 'advances',\n",
              " 699: '21',\n",
              " 700: 'mit',\n",
              " 701: 'press',\n",
              " 702: '2009',\n",
              " 703: 'fast',\n",
              " 704: 'algorithm',\n",
              " 705: 'probabilistic',\n",
              " 706: 'internal',\n",
              " 707: 'back',\n",
              " 708: 'propagating',\n",
              " 709: 'errors',\n",
              " 710: 'dynamic',\n",
              " 711: 'pooling',\n",
              " 712: 'unfolding',\n",
              " 713: 'recursive',\n",
              " 714: 'autoencoders',\n",
              " 715: 'paraphrase',\n",
              " 716: 'detection',\n",
              " 717: 'general',\n",
              " 718: 'semisupervised',\n",
              " 719: 'interna',\n",
              " 720: 'tional',\n",
              " 721: 'artiﬁcial',\n",
              " 722: '2005',\n",
              " 723: 'combining',\n",
              " 724: 'heterogeneous'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ind_wd = []\n",
        "for sen in docs:\n",
        "    for tkn in sen:\n",
        "        if tkn not in ind_wd:\n",
        "            ind_wd.append(tkn)\n",
        "\n",
        "index = {w: idx for (idx, w) in enumerate(ind_wd)}\n",
        "word_ind = {idx: w for (idx, w) in enumerate(ind_wd)}\n",
        "\n",
        "ind_wd_len = len(ind_wd)\n",
        "word_ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyaD9FbvN2_c",
        "outputId": "131ba651-5360-402e-d43f-a2cfadd9112a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   1],\n",
              "       [  0,   2],\n",
              "       [  1,   0],\n",
              "       ...,\n",
              "       [668,   4],\n",
              "       [  4,  43],\n",
              "       [  4, 668]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "window_size = 2\n",
        "idx_pairs = []\n",
        "# for each sentence\n",
        "for sentence in docs:\n",
        "    indices = [index[word] for word in sentence]\n",
        "    # for each word, threated as center word\n",
        "    for center_word_pos in range(len(indices)):\n",
        "        # for each window position\n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            # make soure not jump out sentence\n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "            context_word_idx = indices[context_word_pos]\n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
        "idx_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz4SYTvlfU0W"
      },
      "outputs": [],
      "source": [
        "# def getUniqueWords(wordList):\n",
        "#     unique = set()\n",
        "#     for word in wordList:\n",
        "#       for eac_wd in word:\n",
        "#           unique.add(eac_wd)\n",
        "#     return unique\n",
        "\n",
        "# res = getUniqueWords(docs)\n",
        "# res = list(res)\n",
        "# res = {j:i for i,j in enumerate(res)}\n",
        "# res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r90a2BKX75pF",
        "outputId": "52c8ef12-f3dc-417f-8a09-45c417c145bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['deﬁned', 'transcribed', 'last', 'number', 'approaches', 'choice', 'belongs', 'countries', 'observe', 'dis', 'subtle', 'computation', 'hyperparameter', 'token', 'classiﬁer', 'rest', 'adding', 'paraphrase', 'important', 'robustness', 'indomain', 'compared', 'limits', 'help', 'unrestricted', 'assign', 'show', 'section', 'sets', 'updated', 'similarities', 'developing', 'millions', 'projection', 'initial', 'memory', 'without', 'projected', 'regularities', 'processing', 'range', 'metric', 'accuracy', 'twice', 'position', 'speech', 'increases', 'removed', 'beyond', 'computational', 'linguistic', 'called', 'improves', 'degrees', 'nlp', 'less', 'point', 'four', 'methods', 'method', 'inﬂective', 'minimize', 'within', 'dense', 'values', 'form', 'give', 'keeps', 'hidden', 'brno', 'able', 'small', 'due', 'day', 'represent', 'written', 'named', 'zero', 'distbelief', 'weight', 'belong', 'networks', 'via', 'sense', 'typically', 'relational', 'pthat', 'average', 'derived', 'answered', 'germany', 'focus', 'classiﬁcations', 'good', 'updates', 'center', 'matrix', 'extension', 'architecture', 'centralized', 'explorations', '2008', 'cpu', '1040', 'preserve', 'facing', 'question', 'learn', 'feedforward', 'manually', 'measured', 'billions', 'available', 'advanced', 'subtracting', 'singlemachine', 'follows', '640', 'compute', 'conference', 'biggest', 'operations', 'cbow', 'missing', 'tional', 'continuous', 'composition', 'italy', 'heavily', 'history', 'reasons', 'approach', 'neural', 'averaged', 'global', 'shows', 'artiﬁcial', 'limited', 'server', 'current', 'showing', 'related', 'introduction', 'treat', 'general', 'discoveries', 'language', 'sum', 'term', 'later', 'possibly', 'predict', 'becomes', 'hierarchical', 'better', 'block', 'binary', 'correct', 'means', 'understand', 'encoded', 'connections', 'classiﬁcation', 'tectures4', 'layer', 'context', 'makes', 'take', 'predicts', 'dominated', 'multiple', 'containing', '2010', 'tecture', 'introduced', 'paris', 'mantic', 'past', 'zweig', 'provide', 'successfully', 'train', 'automatic', 'multiword', 'amount', 'operation', 'smaller', 'furthermore', 'used', 'discard', 'provides', 'previous', 'questions', 'similar', 'represented', 'subsets', 'clearly', 'czech', 'layers', 'atomic', 'international', 'thesis', 'obtaining', 'moreover', 'network', 'depends', 'equation', 'speed', 'allows', 'become', '2011', 'go', 'previously', 'usually', 'rrwords', 'synchronizes', 'discuss', 'latent', 'diminishing', 'perplexity', 'represen', 'higly', 'together', 'speedup', 'classify', 'relevant', 'maximize', 'eg', 'different', 'deep', 'two', 'requires', 'empirical', 'subjecting', 'outputs', 'distributed', 'extend', 'criterion', 'found', 'guage', '1000dimensional', 'slightly', 'problem', '3we', 'complete', 'finally', 'innovative', 'notion', 'softmax', 'problems', 'semantic', 'need', 'example', 'distant', 'cognition', 'challenging', 'icml', 'languages', 'data', 'bases', 'three', 'city', 'thus', 'even', 'simple', 'studied', 'took', 'various', 'also', 'ﬁnal', 'given', 'internal', 'precisely', 'press', 'epoch', '2005', 'worse', 'architectures', 'active', '25k', 'labels', 'learned', 'xmeasured', 'decreased', 'consists', 'surprising', 'subset', 'american', 'especially', 'vocabulary', 'result', 'magnitude', 'improve', 'accurate', 'task', 'linearly', 'corpus', 'structure', 'formed', '10', 'skipgram', 'replica', 'sampling', 'syntactic', 'random', 'minimizing', 'closer', 'semisupervised', 'basically', 'uni', 'years', 'machines', 'collection', 'deriving', 'works', 'relatively', 'pairs', '100', 'however', 'tbelief', 'select', 'test', 'included', 'based', 'created', 'optimization', 'versions', 'evaluation', 'r\\x022', 'takes', 'unigram', '30k', 'quickly', 'short', 'solve', 'promising', 'learning', 'trillion', 'microstructure', 'evaluate', 'including', 'explore', 'using', 'category', 'huge', 'lot', 'versity', 'us', 'proposed', 'nnlm', 'list', 'berlin', 'performance', 'perhaps', 'follow', 'yet', 'experiments', 'per', 'recursive', 'uniﬁed', 'distance', 'much', 'semanticsyntactic', 'semeval2012', 'framework', 'adaptive', 'look', 'increase', 'parallel', 'publicly', 'ask', 'bination', 'log2', 'probabilistic', 'chose', 'state', 'stateoftheart', 'analysis', 'easy', 'added', 'vcoding', 'enable', 'separately', 'many', 'single', 'com', 'retrieval', 'words', 'type', 'still', 'cheap', 'shared', 'typical', 'attractive', 'technology', 'highquality', 'long', 'france', 'implemented', 'rnn', 'explored', 'predictions', 'higher', 'starting', 'natural', 'popular', 'ie', 'way', 'described', 'indices', 'translation', 'normalization', 'choose', 'amounts', 'version', 'picking', 'huffman', 'tests', 'lower', 'similarity', 'lda', 'perform', 'improvements', 'joint', 'multitask', 'gets', 'observed', 'although', 'fast', 'complex', 'use', 'gradient', 'correctly', 'achieved', 'mentioned', 'second', 'possible', 'coherent', 'basic', 'trained', 'balanced', 'simpler', 'work', 'evaluated', 'bottleneck', 'applications', 'machine', 'certain', 'search', 'relationship', 'ﬁve', 'estimating', 'expect', 'earlier', 'dynamic', 'detection', 'building', 'kind', 'smallest', 'likely', 'obtained', 'dirichlet', 'restricted', 'score', 'techniques', 'bigis', 'overhead', 'among', 'orders', 'facts', 'linear', 'n\\x02d', '8869', 'dimensional', 'entities', 'computing', 'caused', 'contain', 'systems', 'weeks', 'modelling', 'run', 'relationships', 'around', 'nonlinear', 'units', 'closest', 'multithreaded', 'n\\x02d\\x02hterm', 'order', 'training', 'proc', 'extensions', 'table', 'vectors', 'reported', 'pooling', 'hundred', 'high', '16', 'another', 'online', 'next', 'graphical', 'dimensions', 'step', 'try', 'applied', 'end', 'stochastic', 'design', 'answering', 'connects', 'selecting', 'lms', 'observation', 'impossible', 'big', 'positions', 'unknown', 'representations', 'compositionality', 'loglinear', 'unfolding', 'believe', 'se', 'compare', 'steps', 'one', 'types', 'outofthelist', 'surrounding', 'errors', 'frequent', 'progress', 'providing', 'decrease', 'made', 'signiﬁcant', 'community', '0025', 'part', 'intelligence', 'set', 'interna', 'ﬁrst', 'word', 'improving', 'vector', 'first', 'recent', 'statistical', 'representation', 'development', 'estimate', 'reasonable', 'decided', 'measuring', 'published', 'examples', 'goal', 'results', 'performing', 'larger', 'choices', 'could', 'increasing', 'best', 'future', 'get', 'incorporating', 'mit', 'lsa', 'sentence', 'hour', 'reaching', 'trees', 'simplicity', 'news', 'inputs']\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into train and test sets\n",
        "# Your code\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(word_ind, test_size = 0.2)\n",
        "print(train)\n",
        "\n",
        "# Build the dataloader\n",
        "# Your code\n",
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=16) #todo\n",
        "test_dataloader =  torch.utils.data.DataLoader(test, batch_size=16) #todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MesPKrgT75pF"
      },
      "source": [
        "**9: What is the difference between training and test datasets? [1 Point]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltBdPfvl75pF"
      },
      "source": [
        "**Train**: This data collection will be used to train the model. The model will be trained repeatedly on the same data in our training set at each epoch in order to understand the features of the data.\n",
        "\n",
        "**Test**: It is seperate from training dataset. After the model has been trained, this collection of data is utilized to test the model. Test datasets are typically unlabeled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsjjdlMW75pF"
      },
      "source": [
        "**10:  What are additional factors to consider within IR when splitting datasets into training and test data? [1 Point]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zihSOIqp75pG"
      },
      "source": [
        "[your answer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FJdh1Wn75pG"
      },
      "source": [
        "**11: Implement a training and test loop for the SkipGramModel. [7 Points]**\n",
        "\n",
        "**a.) Training loop [4 Points]**\n",
        "\n",
        "We would like to see the following within your training loop:\n",
        "* select current token and target\n",
        "* convert token and target into tensors\n",
        "* use model to predict target using token\n",
        "* compute train loss with loss function\n",
        "* backpropagation \n",
        "* for every epoch output: train loss\n",
        "\n",
        "For one training step we always need $2$ words: the center word (token) and the context word (target). This is done for each word in a document using a context window with a size of $c=2$ (2 left, 2 right) and has to be done for the whole context window, as an example:\n",
        "$document = [w0,w1,w2,w3,w4,w5]$, current token = $w3$ -> results in: $[(w3,w1),(w3,w2),(w3,w4),(w3,w5)]$\n",
        "\n",
        "\n",
        "**b.) Test loop [3 Points]**\n",
        "\n",
        " We would like to see the following within your training loop:\n",
        " * calculate correctly predicted values (accuracy)\n",
        " * compute test loss (use the loss function from the training loop)\n",
        " * for every epoch output: test loss, test accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7T2ZtXM75pG"
      },
      "outputs": [],
      "source": [
        "EMBED_DIMENSION = 25\n",
        "EMBED_MAX_NORM = 1\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Skip-Gram model described in paper:\n",
        "    https://arxiv.org/abs/1301.3781\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=EMBED_DIMENSION,\n",
        "            max_norm=EMBED_MAX_NORM,\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=EMBED_DIMENSION,\n",
        "            out_features=vocab_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs_):\n",
        "        x = self.embeddings(inputs_)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "vocab_size = 724 # Change to your vocab length(train+test)\n",
        "model = SkipGramModel(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aolVlx6d75pG"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "   size = len(dataloader.dataset)\n",
        "   model.train()\n",
        "   for batch, (X, y) in enumerate(dataloader):\n",
        "     X, y = X.to(device), y.to(device)\n",
        "\n",
        "     pred = model(X)\n",
        "     loss = loss_fn(pred, y)\n",
        "\n",
        "     optimizer.zero_grad()\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "\n",
        "     if batch % 100 == 0:\n",
        "       loss, current = loss.item(), batch * len(X)\n",
        "       print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "   return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzakjxL175pG"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "gBgSBGI075pG",
        "outputId": "64ab7311-fb7a-495b-a38f-7e64b4317562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-ba0fca114f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-1a6af892621b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m    \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m      \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.025)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUx-Xgln75pH"
      },
      "source": [
        "**12: When training, what does the model(SkipGram) try to predict?  [1 Point]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGjnfkA75pH"
      },
      "source": [
        "Predict the words in the context around the target word.\n",
        "\n",
        "**Input:** Each target word is entered as a single-hot vector.\n",
        "\n",
        "**Hidden Layer:** Each one-hot vector should be averaged after being mapped to a dense E-dimensional vector in the hidden layer.\n",
        "\n",
        "**Output:** use softmax to forecast the context word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVfYpVT675pH"
      },
      "source": [
        "**13: How would you recognise overfitting? What are possible ways to prevent it?  [1 Point]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kobue1U-75pH"
      },
      "source": [
        "\n",
        "\n",
        "*   When your model fits well to the training data but does not generalize well to brand-new, untried data, you have overfitted your model. In other words, the model discovered patterns unique to the training set that are unimportant in other sets of data.\n",
        "\n",
        "Possible ways to prevent it are:\n",
        "\n",
        "1.   Adding more data\n",
        "2.   Eliminating some layers from the model or reducing the number of layers would **reduce the complexity of the model**.\n",
        "3.   enabling **dropout** features that allow training to ignore a particular group of neuros or nodes in a layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOxf_W3175pH"
      },
      "source": [
        "## Neural Retrieval - Create your own neural network\n",
        "\n",
        "In the following tasks, you have to implement your own neural network (NN). This NN takes two inputs, namely a query and a document,\n",
        "for which your NN has to predict the relevance between these 2 inputs.\n",
        "\n",
        "Therefore, you are give a set of predefined query and document vectors. The dataset is already split into training and test sets.\n",
        "The relevance indicators are given for the training set(1=relevant, 0=irrelevant).\n",
        "\n",
        "Furthermore, the given relevance set in $df\\_qrels$ for the training data is determined with an unknown relevance\n",
        "function(non-trivial linear combination with a small amount of noise).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdONiRvb75pH"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "df_train_queries = pd.read_csv(\"/content/drive/MyDrive/Course/AIR/data/input_data/train_queries.csv\")\n",
        "df_train_docs = pd.read_csv(\"/content/drive/MyDrive/Course/AIR/data/input_data/train_docs.csv\")\n",
        "df_train_qrels = pd.read_csv(\"/content/drive/MyDrive/Course/AIR/data/input_data/train_qrels.csv\")\n",
        "\n",
        "df_test_queries = pd.read_csv(\"/content/drive/MyDrive/Course/AIR/data/input_data/test_queries.csv\")\n",
        "df_test_docs = pd.read_csv(\"/content/drive/MyDrive/Course/AIR/data/input_data/test_docs.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16eUuwif75pH"
      },
      "source": [
        "**14: Create your model. [2 Points]**\n",
        "\n",
        "In this section you should define your NN model. Take in consideration that your model should receive two inputs, namely a document and query.\n",
        "\n",
        "Also define your $loss function$ and $optimizer$.\n",
        "\n",
        "Again, for further information check out: [Pytorch Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfxDzDhe75pI"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "device = \"cpu\"\n",
        "\n",
        "# Define model\n",
        "class YourModel(nn.Module):\n",
        "    # Your code\n",
        "    def __init__(self):\n",
        "      super(YourModel, self).__init__()\n",
        "      self.conv = nn.Conv2d( ... )  # set up your layer here\n",
        "      self.fc1 = nn.Linear( ... )  # set up first FC layer\n",
        "      self.fc2 = nn.Linear( ... )\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "      c = self.conv(input1)\n",
        "      f = self.fc1(input2)\n",
        "      # now we can reshape `c` and `f` to 2D and concat them\n",
        "      combined = torch.cat((c.view(c.size(0), -1),\n",
        "                            f.view(f.size(0), -1)), dim=1)\n",
        "      out = self.fc2(combined)\n",
        "    pass\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "model = YourModel().to(device)\n",
        "\n",
        "# Your code\n",
        "optimizer = None #todo\n",
        "loss_fn = None #todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA5OT30275pI"
      },
      "source": [
        "**15: Create dataloader. [1 Point]**\n",
        "\n",
        "For the model, we would recommend using a dataloader to zip queries, documents and relevance into one list. This would then look like this: $[(d_0,q_0,r_0),..(d_n,q_n,r_n)]$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrMJgnhb75pI"
      },
      "outputs": [],
      "source": [
        "#create dataloader\n",
        "train_dataloader = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcp74reJ75pI"
      },
      "source": [
        "**16: Implement your training function. [4 Points]**\n",
        "\n",
        "The training function is required to have:\n",
        "* Calculate the loss based on the prediction of the model\n",
        "* Apply backpropagation, update optimizer\n",
        "* Output $F_1$ and $loss$ every 1000 batches\n",
        "* Return the total loss at the end\n",
        "\n",
        "_Hint_: make your predictions based on the document and query. Furthermore, compute the loss by using the given relevance in $df\\_qrels$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFR6nbem75pI"
      },
      "outputs": [],
      "source": [
        "# Train function\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    # Your code\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "moSpcJlq75pI",
        "outputId": "923e05f7-ba72-464c-e001-229d74dc2959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-ab8dc77fc458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_fn' is not defined"
          ]
        }
      ],
      "source": [
        "# Run model\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJorRqkB75pI"
      },
      "source": [
        "**17: Output predictions for the test set. [1 Point]**\n",
        "\n",
        "Use the trained model to predict the relevance for all documents and queries in the test dataset. Use a threshold of $t = 0.5$ to decide if a query is relevant or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY3JrvwS75pJ"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}